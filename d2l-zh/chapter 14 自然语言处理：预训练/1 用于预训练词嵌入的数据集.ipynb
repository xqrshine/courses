{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc3e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d08e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip', \n",
    "                      '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "def read_ptb():\n",
    "    \"\"\"将PTB数据集加载到文本行的列表中。\"\"\"\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Read the training set.\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea9660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences数：42069\n"
     ]
    }
   ],
   "source": [
    "print(f\"sentences数：{len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5474cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:6719\n"
     ]
    }
   ],
   "source": [
    "# 为语料库构建一个词表，将出现次数少于10次的单词都用‘<unk>’词元替换\n",
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "print(f'vocab size:{len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1bb07",
   "metadata": {},
   "source": [
    "下采样,数据集中每个词将有概率地被丢弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd704d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(sentences, vocab):\n",
    "    \"\"\"下采样高频词\"\"\"\n",
    "    # 排除未知词元'<unk>'\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                for line in sentences]\n",
    "    counter = d2l.count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "    \n",
    "        # 如果在下采样期间保留词元，则返回True\n",
    "    def keep(token):\n",
    "        return (random.uniform(0, 1) < \n",
    "                math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "    \n",
    "    return ([[token for token in line if keep(token)] \n",
    "             for line in sentences], counter)\n",
    "\n",
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a591952",
   "metadata": {},
   "source": [
    "下采样后，将词元映射到它们在语料库中的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e69dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [vocab[line] for line in subsampled]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b108fe",
   "metadata": {},
   "source": [
    "提取中心词和上下文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c305f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"返回跳远模型中的中心词和上下文词\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # 要形成 ‘中心词-上下文词’对，每个句子至少需要2个词\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)): # 上下文窗口中间‘i’\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size), \n",
    "                                 min(len(line), i + 1 + window_size))) # 上下文窗口下标\n",
    "            # 从上下文词中排除中心词\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a94de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center-context pairs:1500778\n"
     ]
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "print(f'center-context pairs:{sum([len(contexts) for contexts in all_contexts])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045df287",
   "metadata": {},
   "source": [
    "负采样进行近似训练，采样k个不是来自上下文窗口的噪声词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f488b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18513919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于一对中心词和上下文词，随机抽取K个（例如5个）噪声词\n",
    "# 一对是指：中心词和上下文窗口中的一个词 是一对。\n",
    "all_negatives = d2l.get_negatives(all_contexts, vocab, counter, 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b5e9d",
   "metadata": {},
   "source": [
    "小批量加载训练实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad57db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"返回带有负采样的跳远模型的小批量\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(contexts_negatives),\n",
    "           torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda3e0b",
   "metadata": {},
   "source": [
    "最后，定义读取PTB数据集并返回数据迭代器和词表的load_data_ptb函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "892c148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_ptb(batch_size, max_window_size, num_niose_words):\n",
    "    \"\"\"下载PTB数据集，然后将其加载到内存中\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = d2l.get_negatives(all_contexts, vocab, counter, num_niose_words) \n",
    "    \n",
    "    class PTBDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives)\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index],\n",
    "                   self.negatives[index])\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "        \n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, \n",
    "                                               batch_size, \n",
    "                                               shuffle=True,\n",
    "                                                  collate_fn=batchify,\n",
    "                                                  num_workers=num_workers)\n",
    "    return data_iter, vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149c2c0",
   "metadata": {},
   "source": [
    "打印一个小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4aa060f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "data_iter, vocab = load_data_ptb(512, 5, 5)\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a2b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "d2l-zh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
