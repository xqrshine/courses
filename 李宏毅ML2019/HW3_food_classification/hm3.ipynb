{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "    此次資料集為網路上蒐集到的食物照片，共有11類\n",
    "    Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
    "    Training set: 9866張\n",
    "    Validation set: 1451張\n",
    "    Testing set: 3347張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"，这一句必须加在import torch之前，紧跟import os之后。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read image\n",
    "    利用opencv(cv2)读入图片并放在numpy array中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    \"\"\"\n",
    "    path:数据集的文件夹地址\n",
    "    label:一個 boolean variable，代表需不需要回傳 y 值 ??不懂什么叫做需不需要回传y值\n",
    "    （回传y,就是下面的数据集需要y值的时候，一起将y值也返回，比如training set, validation set）\n",
    "    \"\"\"\n",
    "    \n",
    "    image_dir = sorted(os.listdir(path))  # 获取文件夹中的文件列表，并将文件名排序\n",
    "    \n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        image = cv2.imread(os.path.join(path, file))  # 读取图片的数值\n",
    "        if image is None:\n",
    "            continue\n",
    "        x[i,:,:] = cv2.resize(image,(128, 128))  # 图片缩放为128*128的方形图像\n",
    "        if label:\n",
    "            y[i] = int(file.split('_')[0])  # 获取图像的label标签\n",
    "    if label:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data: \n",
      "Size of training data = 9866\n",
      "Size of validation data = 1451\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分别将training set, validation set, testing set用readfile函数读进来\n",
    "workspace_dir = \"./food-11\"\n",
    "print(\"Reading data: \")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "数据增强，增加数据的数量，阻止神经网络学习不相关特征\n",
    "    \n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset(数据集抽象类) 及 DataLoader（数据加载器） 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：__len__ 及 __getitem__\n",
    "\n",
    "__len__ 必須要回傳 dataset 的大小，而 __getitem__ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置转换格式\n",
    "# training时做data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 将shape为(C,H,W)的Tensor或shape为(H,W,C)的numpy.ndarray转换成PIL.Image，值不变。\n",
    "    transforms.RandomHorizontalFlip(),  # 随机将图片水平翻转  \n",
    "    transforms.RandomRotation(15),  # 随机旋转图片\n",
    "    transforms.ToTensor()  # 将图片转成Tensor,并把数值normalize到[0,1](data normalization)\n",
    "])\n",
    "# testing时做data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "class ImgDataset(Dataset):  \n",
    "    \"\"\"创建img数据集的类，继承自Dataset抽象类\"\"\"\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        \"\"\"\n",
    "        重写初始化函数，定义变量x,y,transform\n",
    "        \"\"\"\n",
    "        self.x  = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        重写__len__方法\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        重写__getitem__方法\n",
    "        \"\"\"\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = ImgDataset(train_x, train_y, train_transform)  # 训练集\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)  # 验证集（源码为什么要进行test_transform?）\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)  # 每个epoch都打乱顺序什么意思？\n",
    "# 因为读取数据的时候是排序的，所以读取出的数据集的label也是有序的\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化网络结构模块\n",
    "        \"\"\"\n",
    "        # 继承自基类\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 维度 [3, 128, 128]\n",
    "        # 定义隐藏层结构\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 五层卷积层\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128,]\n",
    "            # 归一化，使得在做Relu的时候不会因为数据过大而导致网络不稳定，参数通常是通道数 \n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  # [64, 64, 64]\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),  # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  # [128, 32, 32]\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),  # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  # [256, 16, 16]\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),  # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1),  # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)  # [512, 4, 4]\n",
    "        )\n",
    "        # 定义输出层线性变换，fc的分类网络\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),  # 对输入数据做线性变换（输入样本特征值的大小， 输出样本特征值的大小）\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 11)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)  # 将卷积后的图像矩阵拉直成向量\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "model.train():让模型编程训练模式，此时dropout和Batch Normalization 在训练的时候起到防止网络过拟合的作用\n",
    "\n",
    "model.eval():pytorch会自动把BN 和 dropout固定住，不会取平均值，而是训练好的值，\n",
    "\n",
    "网上的一个问题：请问一下 pytorch 模型的eval模式比train模式的效果差很多 （bn层导致的） 应该怎么解决呢？\n",
    "\n",
    "答案：说明过拟合了，测试的时候用eval模式，如果测试的时候开成train模式，dropout会起作用，bn参数会改变（根据测试数据），测试效果就会变差。\n",
    "\n",
    "所以不开eval模式效果会变差，如果开了eval模式test效果很差，那就是过拟合了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 29.50 sec(s) Train Acc: 0.215690 Loss: 0.018544 | Val Acc: 0.115782 loss: 0.018633\n",
      "[002/030] 23.52 sec(s) Train Acc: 0.322623 Loss: 0.015025 | Val Acc: 0.339766 loss: 0.016480\n",
      "[003/030] 28.58 sec(s) Train Acc: 0.384857 Loss: 0.013807 | Val Acc: 0.360441 loss: 0.018917\n",
      "[004/030] 23.76 sec(s) Train Acc: 0.424083 Loss: 0.013039 | Val Acc: 0.307374 loss: 0.016808\n",
      "[005/030] 26.00 sec(s) Train Acc: 0.448814 Loss: 0.012522 | Val Acc: 0.366644 loss: 0.014795\n",
      "[006/030] 22.70 sec(s) Train Acc: 0.490675 Loss: 0.011589 | Val Acc: 0.331496 loss: 0.016848\n",
      "[007/030] 24.84 sec(s) Train Acc: 0.519765 Loss: 0.010964 | Val Acc: 0.421089 loss: 0.014235\n",
      "[008/030] 26.08 sec(s) Train Acc: 0.535171 Loss: 0.010514 | Val Acc: 0.414197 loss: 0.013803\n",
      "[009/030] 28.92 sec(s) Train Acc: 0.568620 Loss: 0.009839 | Val Acc: 0.534114 loss: 0.011428\n",
      "[010/030] 21.16 sec(s) Train Acc: 0.591932 Loss: 0.009296 | Val Acc: 0.368711 loss: 0.016856\n",
      "[011/030] 22.38 sec(s) Train Acc: 0.604399 Loss: 0.008960 | Val Acc: 0.533425 loss: 0.010830\n",
      "[012/030] 25.64 sec(s) Train Acc: 0.629434 Loss: 0.008370 | Val Acc: 0.422467 loss: 0.015433\n",
      "[013/030] 22.49 sec(s) Train Acc: 0.639368 Loss: 0.008258 | Val Acc: 0.518263 loss: 0.010656\n",
      "[014/030] 24.86 sec(s) Train Acc: 0.668153 Loss: 0.007491 | Val Acc: 0.578222 loss: 0.010742\n",
      "[015/030] 23.19 sec(s) Train Acc: 0.686398 Loss: 0.007126 | Val Acc: 0.570641 loss: 0.010603\n",
      "[016/030] 24.21 sec(s) Train Acc: 0.702818 Loss: 0.006779 | Val Acc: 0.560303 loss: 0.011280\n",
      "[017/030] 21.83 sec(s) Train Acc: 0.712041 Loss: 0.006564 | Val Acc: 0.581668 loss: 0.010349\n",
      "[018/030] 24.77 sec(s) Train Acc: 0.717616 Loss: 0.006452 | Val Acc: 0.578222 loss: 0.011145\n",
      "[019/030] 23.25 sec(s) Train Acc: 0.722785 Loss: 0.006251 | Val Acc: 0.574776 loss: 0.010680\n",
      "[020/030] 26.66 sec(s) Train Acc: 0.750862 Loss: 0.005624 | Val Acc: 0.501723 loss: 0.013403\n",
      "[021/030] 24.68 sec(s) Train Acc: 0.760085 Loss: 0.005403 | Val Acc: 0.537560 loss: 0.013449\n",
      "[022/030] 22.72 sec(s) Train Acc: 0.766065 Loss: 0.005214 | Val Acc: 0.517574 loss: 0.012622\n",
      "[023/030] 23.64 sec(s) Train Acc: 0.764748 Loss: 0.005282 | Val Acc: 0.579600 loss: 0.010847\n",
      "[024/030] 25.28 sec(s) Train Acc: 0.792216 Loss: 0.004596 | Val Acc: 0.581668 loss: 0.012197\n",
      "[025/030] 23.38 sec(s) Train Acc: 0.803669 Loss: 0.004453 | Val Acc: 0.576844 loss: 0.011342\n",
      "[026/030] 21.94 sec(s) Train Acc: 0.809649 Loss: 0.004229 | Val Acc: 0.606478 loss: 0.011699\n",
      "[027/030] 21.42 sec(s) Train Acc: 0.830022 Loss: 0.003882 | Val Acc: 0.507926 loss: 0.017100\n",
      "[028/030] 23.54 sec(s) Train Acc: 0.810055 Loss: 0.004275 | Val Acc: 0.510682 loss: 0.014082\n",
      "[029/030] 24.94 sec(s) Train Acc: 0.841678 Loss: 0.003531 | Val Acc: 0.590627 loss: 0.011756\n",
      "[030/030] 25.95 sec(s) Train Acc: 0.858909 Loss: 0.003246 | Val Acc: 0.623019 loss: 0.011820\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss()  #  分类任务，loss是使用交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 优化函数用Adam\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train()  # 确保model是在train model (开放Dropout等…)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # 用optimizer将model参数的gradient清零\n",
    "        train_pred = model(data[0].cuda())  # 利用model得到预测的概率分布，这里实际上就是去调用model的forward函数\n",
    "        batch_loss = loss(train_pred, data[1].cuda())  # 计算loss,(注意prediction跟label必须同时在cpu或者gpu上)\n",
    "        batch_loss.backward()  # 利用back propagation算出每个参数的gradient\n",
    "        optimizer.step()  # 以optimizer用gradient更新参数值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set 和 val_set一起训练，训练数据越多，精确度越高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7833346420a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Classifier' is not defined"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model_best.parameters(), lr=0.001)\n",
    "epoches = 30\n",
    "\n",
    "print('iter, \\tduration, \\ttrain_val_acc, \\ttrain_val_loss')\n",
    "for epoch in range(epoches):\n",
    "    epoch_start_time = time.time()\n",
    "    train_val_acc = 0.0\n",
    "    train_val_loss = 0.0\n",
    "    model_best.train()\n",
    "    for i,data in enumerate(train_val_loader):\n",
    "        optim.zero_grad()\n",
    "        train_val_predict = model_best(data[0].cuda())\n",
    "        batch_loss = loss_func(train_val_predict, data[1].cuda())\n",
    "        optim.step()\n",
    "        \n",
    "        train_val_acc += np.sum(np.argmax(train_val_predict.data.numpy(), axis=1) == data[1].cuda())\n",
    "        train_val_loss += batch_loss.item()\n",
    "    print('{}, \\t{} sec(s), \\t{:.4f}, \\t{:.4f}'.format(epoch, time.time() - epoch_start_time, train_val_acc, train_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test\n",
    "利用刚刚train好的model进行prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.76 GiB total capacity; 3.98 GiB already allocated; 201.56 MiB free; 262.33 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3007c6307047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 怎么理解.data呢，module返回的是个什么样的值呢？tensor?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b2ec5af011f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将卷积后的图像矩阵拉直成向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/venv-courses_ML19/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.76 GiB total capacity; 3.98 GiB already allocated; 201.56 MiB free; 262.33 MiB cached)"
     ]
    }
   ],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predict = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_predict = model(data.cuda())\n",
    "        # 怎么理解.data呢，module返回的是个什么样的值呢？tensor?\n",
    "        test_label = np.argmax(test_predict.cpu().data.numpy(), axis=1)  \n",
    "        for y in test_label:\n",
    "            predict.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预测结果写入test_out.csv文件\n",
    "with open('test_out.csv', 'w') as file:\n",
    "    file.write('Id, \\tCategory\\n')\n",
    "    for i, y in enumerate(predict):\n",
    "        file.write('{}, \\t{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***因为GPU内存不够的原因，在第一步训练的时候就占用了5277MB内存，以致于第二次的训练和test时内存不够用，大致就是这样啦。**\n",
    "\n",
    "**会了第一次的训练，第二次训练和第一次差不多，测试的时候的原理也差不多**\n",
    "\n",
    "**谢谢注释和文档，第三次作业就算是结束啦！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-courses_ML19",
   "language": "python",
   "name": "python3-courses_ml19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
