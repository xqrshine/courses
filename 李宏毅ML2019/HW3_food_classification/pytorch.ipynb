{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(446)\n",
    "np.random.seed(446)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and relation to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_tensor\n",
      "[0.1  0.22 0.3 ] tensor([0.1000, 0.2000, 0.3000])\n",
      "\n",
      "to and from numpy and pytorch\n",
      "tensor([0.1000, 0.2200, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
      "\n",
      "x+y\n",
      "[3.1  4.22 5.3 ] tensor([3.1000, 4.2000, 5.3000])\n",
      "\n",
      "norm\n",
      "0.38522720568516444 tensor(0.3742)\n",
      "\n",
      "mean along the 0th dimension\n",
      "[2. 3.] tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "# create tensors in a similar way to numpy and arrays\n",
    "x_numpy = np.array([0.1, 0.22, 0.3])\n",
    "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
    "print('x_numpy, x_tensor')\n",
    "print(x_numpy, x_torch)\n",
    "print()\n",
    "\n",
    "# to and from numpy, pytorch\n",
    "print('to and from numpy and pytorch')\n",
    "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
    "print()\n",
    "\n",
    "# we can do basic operations like +-*/\n",
    "y_numpy = np.array([3, 4, 5.])\n",
    "y_torch = torch.tensor([3, 4, 5.])\n",
    "print('x+y')\n",
    "print(x_numpy + y_numpy, x_torch + y_torch)\n",
    "print()\n",
    "\n",
    "# many functions that are in numpy sre also in pytorch\n",
    "print('norm')\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
    "print()\n",
    "\n",
    "# to apply an operations along a dimension,\n",
    "# we use the dim keyword argment instead of axis\n",
    "print('mean along the 0th dimension')\n",
    "x_numpy = np.array([[1,2], [3,4]])\n",
    "x_tensor = torch.tensor([[1,2], [3,4]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor.view\n",
    "We can reshape tensors similarly to numpy.reshape()\n",
    "\n",
    "It can automatically calculate the correct dimension if a -1 is passed in.This is useful if we are working with batch size is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 28, 28])\n",
      "torch.Size([10000, 3, 784])\n",
      "torch.Size([10000, 3, 784])\n"
     ]
    }
   ],
   "source": [
    "# \"MNIST\"\n",
    "N, C, W, H = 10000, 3, 28, 28\n",
    "X = torch.randn(N, C, W, H)\n",
    "print(X.shape)\n",
    "print(X.view(N, C, 784).shape)\n",
    "print(X.view(-1, C, 784).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(3., grad_fn=<AddBackward0>)\n",
      "d tensor(2., grad_fn=<AddBackward0>)\n",
      "e tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d\n",
    "print('c', c)\n",
    "print('d', d)\n",
    "print('e', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SEMANTICS\n",
    "It's easy cupy tensor from cpu to gpu from gpu to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3360, 0.0470, 0.8528, 0.8159, 0.6722, 0.1176, 0.8227, 0.3750, 0.7711,\n",
      "        0.7036])\n",
      "tensor([0.3360, 0.0470, 0.8528, 0.8159, 0.6722, 0.1176, 0.8227, 0.3750, 0.7711,\n",
      "        0.7036], device='cuda:0')\n",
      "tensor([0.3360, 0.0470, 0.8528, 0.8159, 0.6722, 0.1176, 0.8227, 0.3750, 0.7711,\n",
      "        0.7036])\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda\")\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x = x.to(gpu)\n",
    "print(x)\n",
    "x = x.to(cpu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTroch as an auto grad framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch'sf'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))\n",
    "print('PyTorch\\'sf\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical grad g(w) tensor([2.0000, 5.2832])\n",
      "PyTorch's grad g(w) tensor([2.0000, 5.2832])\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
    "\n",
    "def grad_g(w):\n",
    "    return torch.tensor([2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0])])\n",
    "\n",
    "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
    "z = g(w)\n",
    "z.backward()\n",
    "\n",
    "print('Analytical grad g(w)', grad_g(w))\n",
    "print('PyTorch\\'s grad g(w)', w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tf(x), \tf'(x), \tf'(x) pytorch\n",
      "0, \t5.000, \t9.000, \t6.000,\t6.000\n",
      "1, \t3.500, \t2.250, \t3.000,\t3.000\n",
      "2, \t2.750, \t0.562, \t1.500,\t1.500\n",
      "3, \t2.375, \t0.141, \t0.750,\t0.750\n",
      "4, \t2.188, \t0.035, \t0.375,\t0.375\n",
      "5, \t2.094, \t0.009, \t0.188,\t0.188\n",
      "6, \t2.047, \t0.002, \t0.094,\t0.094\n",
      "7, \t2.023, \t0.001, \t0.047,\t0.047\n",
      "8, \t2.012, \t0.000, \t0.023,\t0.023\n",
      "9, \t2.006, \t0.000, \t0.012,\t0.012\n",
      "10, \t2.003, \t0.000, \t0.006,\t0.006\n",
      "11, \t2.001, \t0.000, \t0.003,\t0.003\n",
      "12, \t2.001, \t0.000, \t0.001,\t0.001\n",
      "13, \t2.000, \t0.000, \t0.001,\t0.001\n",
      "14, \t2.000, \t0.000, \t0.000,\t0.000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "print('iter, \\tf(x), \\tf\\'(x), \\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward()  # compute the gradient\n",
    "    print('{}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "\n",
    "    x.data = x.data - step_size * x.grad  # perform a GD update step\n",
    "          \n",
    "    # We need to zero the grad variable since the bachward()\n",
    "    # call accumulates the gradient in .grad instead of overwriting\n",
    "    # The detach_() if for efficiency. You do not to worry too much about it.\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Now, instead of minimizing a made-up function, lets minimize a loss function on some made-up data.\n",
    "\n",
    "We will implement Gradient Descent in order to sovle the task of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape torch.Size([50, 2])\n",
      "y shape torch.Size([50, 1])\n",
      "w shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# make a simple linea dataset with some noise\n",
    "# 50个样本点，2个特征值\n",
    "d = 2\n",
    "n = 50\n",
    "X = torch.randn(n, d)\n",
    "true_w = torch.tensor([[-1.0], [2.0]])\n",
    "y = X @ true_w + torch.randn(n, 1) * 0.1\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "print('w shape', true_w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_w \\mathcal{L}_{RSS}(w; X) = \\nabla_w\\frac{1}{n} ||y - Xw||_2^2 = -\\frac{2}{n}X^T(y-Xw)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算gradient [ 2.7337537 -3.8884919]\n",
      "pytorch gradient [ 2.7337537 -3.8884923]\n"
     ]
    }
   ],
   "source": [
    "# 定义rss损失函数\n",
    "# define a linear model with no bias\n",
    "def model(X, w):\n",
    "    return X @ w\n",
    "\n",
    "# the residual sum of squares loss function\n",
    "def rss(y, y_hat):\n",
    "    return torch.norm(y - y_hat)**2 / n\n",
    "\n",
    "# analytical expression for the gradient\n",
    "def grad_rss(X, y, w):\n",
    "    return -2*X.t() @ (y - X @ w) / n\n",
    "\n",
    "w = torch.tensor([[1.], [0]], requires_grad=True)\n",
    "y_hat = model(X, w)\n",
    "\n",
    "loss = rss(y, y_hat)\n",
    "loss.backward()\n",
    "\n",
    "print('计算gradient', grad_rss(X, y, w).detach().view(2).numpy())\n",
    "print('pytorch gradient', w.grad.view(2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss, \tw\n",
      "0, \t6.61, \t[0.45324928 0.77769846]\n",
      "1, \t2.88, \t[0.25240588 1.0125588 ]\n",
      "2, \t2.01, \t[0.07887919 1.2013885 ]\n",
      "3, \t1.41, \t[-0.07085963  1.3533686 ]\n",
      "4, \t0.99, \t[-0.19992636  1.4758236 ]\n",
      "5, \t0.70, \t[-0.3110618  1.5746008]\n",
      "6, \t0.50, \t[-0.40666887  1.6543715 ]\n",
      "7, \t0.36, \t[-0.48884836  1.7188704 ]\n",
      "8, \t0.25, \t[-0.55943227  1.7710859 ]\n",
      "9, \t0.18, \t[-0.62001455  1.8134109 ]\n",
      "10, \t0.13, \t[-0.6719795  1.8477634]\n",
      "11, \t0.10, \t[-0.716527   1.8756822]\n",
      "12, \t0.07, \t[-0.75469553  1.8984028 ]\n",
      "13, \t0.05, \t[-0.78738266  1.9169185 ]\n",
      "14, \t0.04, \t[-0.81536305  1.9320284 ]\n",
      "15, \t0.03, \t[-0.8393047  1.9443762]\n",
      "16, \t0.02, \t[-0.8597829  1.9544811]\n",
      "17, \t0.02, \t[-0.8772926  1.9627622]\n",
      "18, \t0.02, \t[-0.8922593  1.9695585]\n",
      "19, \t0.01, \t[-0.90504867  1.9751439 ]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using GD with automatically computed derivatives\n",
    "# 使用GD和自动计算的导数进行线性回归\n",
    "step_size = 0.1\n",
    "print('iter, \\tloss, \\tw')\n",
    "for i in range(20):\n",
    "    y_hat = model(X, w)\n",
    "    loss = rss(y, y_hat)\n",
    "    \n",
    "    loss.backward()  # compute the gradient of the loss\n",
    "    w.data = w.data - step_size * w.grad  # do a gradient descent step\n",
    "    print('{}, \\t{:.2f}, \\t{}'.format(i, loss.item(), w.view(2).detach().numpy()))\n",
    "    \n",
    "    w.grad.detach()\n",
    "    w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Module\n",
    "`Module` is PyTorch's way of performing operations on tensors.Modules are implemented as subclasses of the `torch.nn.Module` class.All modules are callable and can be composed together to create complex functions.\n",
    "\n",
    "[`torch.nn` docs](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "Note:most of the functionality implemented for modules can be accessed in a functional form via `torch.nn.functional`, but these require you to create and manage the weight tensors yourself.\n",
    "\n",
    "[`torch.nn.functional` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module\n",
    "The bread and butter of modules is the Linear module which does a linear transformation with a bias. It takes the input and output dimensions as parameters, and creates the weights in the object.\n",
    "\n",
    "Unlike how we initialized our  w  manually, the Linear module automatically initializes the weights randomly. For minimizing non convex loss functions (e.g. training neural networks), initialization is important and can affect results. If training isn't working as well as expected, one thing to try is manually initializing the weights to something different from the default. PyTorch implements some common initializations in torch.nn.init.\n",
    "\n",
    "模块的主要内容是线性模块，它可以在偏置的情况下进行线性变换。 它以输入和输出尺寸为参数，并在对象中创建权重。\n",
    "\n",
    "与我们手动初始化w的方式不同，线性模块会自动随机地初始化权重。 为了最小化非凸损失函数（例如训练神经网络），初始化很重要并且会影响结果。 如果训练效果不如预期，则可以尝试的一件事是将权重手动初始化为与默认值不同的值。 PyTorch在torch.nn.init中实现了一些常见的初始化。\n",
    "\n",
    "[`torch.nn.init` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor torch.Size([2, 3])\n",
      "transformed torch.Size([2, 4])\n",
      "\n",
      "we can see that the weights exist in the background\n",
      "\n",
      "weight Parameter containing:\n",
      "tensor([[ 0.0968,  0.1537, -0.3115],\n",
      "        [-0.5393, -0.3418, -0.5293],\n",
      "        [-0.3164, -0.2906,  0.4814],\n",
      "        [ 0.3724, -0.0847,  0.3947]], requires_grad=True)\n",
      "bais Parameter containing:\n",
      "tensor([-0.0275,  0.2247, -0.3916,  0.5080], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(d_in, d_out)\n",
    "example_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "transformed = linear_module(example_tensor)\n",
    "\n",
    "print('example_tensor', example_tensor.shape)\n",
    "print('transformed', transformed.shape)\n",
    "print()\n",
    "print('we can see that the weights exist in the background\\n')\n",
    "print('weight', linear_module.weight)\n",
    "print('bais', linear_module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activational functions \n",
    "PyTorch implements a number of activation functions including but not limited to ReLU, tanh, and sigmoid.Since they are modules, they need to be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eaxmple_tensor tensor([-1.,  1.,  2.])\n",
      "activated tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.ReLU()\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 2.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print('eaxmple_tensor', example_tensor)\n",
    "print('activated', activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentail\n",
    "Many times, we want to compose Modules together.`torch.nn.Sequential` provides a good interface for composing simple modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed tensor([[0.5078],\n",
      "        [0.4976]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_hidden = 4\n",
    "d_out = 1\n",
    "model = nn.Sequential(nn.Linear(d_in, d_hidden),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(d_hidden, d_out),\n",
    "                     nn.Sigmoid())\n",
    "example_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "transformed = model(example_tensor)\n",
    "print('transformed', transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:We can get all of the parameters (of any nn.Module) with the `parameters()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2920, -0.0713,  0.4625],\n",
      "        [ 0.1687,  0.0713, -0.4099],\n",
      "        [-0.0219, -0.2541,  0.1275],\n",
      "        [-0.0890, -0.5499, -0.1212]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1837,  0.4708, -0.3839, -0.5094], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1713,  0.3758, -0.1733, -0.3311]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3267], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "for param in params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss funcitons\n",
    "PyTorch implements many common loss function including `MSELoss` and `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多的 loss 函数都有 size_average 和 reduce 两个布尔类型的参数。因为一般损失函数都是直接计算 batch 的数据，因此返回的 loss 结果都是维度为 (batch_size, ) 的向量。\n",
    "\n",
    "如果 reduce = False，那么 size_average 参数失效，直接返回向量形式的 loss;\n",
    "\n",
    "如果 reduce = True，那么 loss 返回的是标量.\n",
    "\n",
    "指定这两个参数中的任何一个都将覆盖`reduction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6667)\n"
     ]
    }
   ],
   "source": [
    "mse_loss_fn = nn.MSELoss()\n",
    "example_input = torch.tensor([0., 0, 0])\n",
    "example_target = torch.tensor([1., 2, 3])\n",
    "loss = mse_loss_fn(example_input, example_target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.optim\n",
    "PyTorch implements a number of gradient-based optimization methods in `torch.optim`, including Gradient Descent. At the minimum, it takes in the model parameters and a learning rate.\n",
    "\n",
    "Optimizers do not compute the gradients for you, so you must call `backward()` yourself. You also must call the `optim.zero_grad()` function before calling `backward()` since by default PyTorch does and inplace add to the `.grad` member variable rather than overwriting it.\n",
    "\n",
    "This does both the `detach_()` and `zero_()` calls on all tensor's `grad` variables.\n",
    "\n",
    "[`torch.optim` docs](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "PyTorch在torch.optim中实现了许多基于梯度的优化方法，包括Gradient Descent。 至少要考虑模型参数和学习率。\n",
    "\n",
    "优化器不会为您计算梯度，因此您必须自己调用back（）。 您还必须在调用Backward（）之前调用optim.zero_grad（）函数，因为默认情况下PyTorch会在.grad成员变量中进行并就地添加而不是覆盖它。\n",
    "\n",
    "这对所有张量的grad变量都进行了detach_（）和zero_（）调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params before: Parameter containing:\n",
      "tensor([[-0.6368]], requires_grad=True)\n",
      "model params after: Parameter containing:\n",
      "tensor([[-0.5685]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a simple model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# create a simple dataset\n",
    "X = torch.tensor([1.])\n",
    "y = torch.tensor([2.])\n",
    "\n",
    "# define the loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# create our optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)  # 定义的时候传入参数\n",
    "\n",
    "print('model params before:', model.weight)\n",
    "y_hat = model(X)\n",
    "loss = mse_loss_fn(y, y_hat)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()  # 这里无需传参数\n",
    "print('model params after:', model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics in PyTorch\n",
    "造数据创建一个简单的神经网络\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建200个一维随机数据，y=4*sin(pi*x)*cos(6*pi**2)\n",
    "d = 1\n",
    "n = 200\n",
    "X = torch.rand(n, d)\n",
    "y = 4 * torch.sin(np.pi * X) * torch.cos(6*np.pi*X**2)\n",
    "\n",
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.title('plot of $f(x)$')\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tloss\n",
      "0,\t4.04\n",
      "100,\t3.78\n",
      "200,\t3.76\n",
      "300,\t3.74\n",
      "400,\t3.71\n",
      "500,\t3.69\n",
      "600,\t3.66\n",
      "700,\t3.62\n",
      "800,\t3.55\n",
      "900,\t3.45\n"
     ]
    }
   ],
   "source": [
    "# 创建一个三层的神经网络，在一定的epoches内迭代使用SGD优化算法求解权重\n",
    "step_size = 0.05\n",
    "epoches = 1000\n",
    "hidden_1 = 32\n",
    "hidden_2 = 32\n",
    "out = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(d, hidden_1),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_1, hidden_2),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_2, out))\n",
    "loss_func = nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=step_size)\n",
    "print('iter,\\tloss')\n",
    "for i in range(epoches):\n",
    "    y_hat = model(X)\n",
    "    loss = loss_func(y_hat, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % (epoches // 10) == 0:\n",
    "        print('{},\\t{:.2f}'.format(i, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fb116e633cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'plot of $f(x)$ and $\\hat{f}(x)$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "X_grid = torch.from_numpy(np.linspace(0,1,50).float().view(-1,d))\n",
    "y_hat = model(X)\n",
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), 'r')\n",
    "plt.title('plot of $f(x)$ and $\\hat{f}(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Sidenote: Momentum\n",
    "\n",
    "There are other optimization algorithms besides stochastic gradient descent. One is a modification of SGD called momentum. We won't get into it here, but if you would like to read more [here](https://distill.pub/2017/momentum/) is a good place to start.\n",
    "\n",
    "We only change the step size and add the momentum keyword argument to the optimizer. Notice how it reduces the training loss in fewer iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tloss\n",
      "0,\t4.04\n",
      "100,\t3.43\n",
      "200,\t2.62\n",
      "300,\t0.55\n",
      "400,\t0.22\n",
      "500,\t0.08\n",
      "600,\t0.06\n",
      "700,\t0.05\n",
      "800,\t0.02\n",
      "900,\t0.00\n"
     ]
    }
   ],
   "source": [
    "# momentum:动态改变leaning rate\n",
    "step_size = 0.05\n",
    "momentum = 0.9\n",
    "epoches = 1000\n",
    "hidden_1 = 32\n",
    "hidden_2 = 32\n",
    "out = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(d, hidden_1),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_1, hidden_2),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_2, out))\n",
    "loss_func = nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=step_size, momentum=momentum)\n",
    "print('iter,\\tloss')\n",
    "for i in range(epoches):\n",
    "    y_hat = model(X)\n",
    "    loss = loss_func(y_hat, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % (epoches // 10) == 0:\n",
    "        print('{},\\t{:.2f}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "So far, we have been considering regression tasks and have used the [MSELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss) module. For the homework, we will be performing a classification task and will use the cross entropy loss.\n",
    "\n",
    "PyTorch implements a version of the cross entropy loss in one module called [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). Its usage is slightly different than MSE, so we will break it down here. \n",
    "\n",
    "- input: The first parameter to CrossEntropyLoss is the output of our network. It expects a *real valued* tensor of dimensions $(N,C)$ where $N$ is the minibatch size and $C$ is the number of classes. In our case $N=3$ and $C=2$. The values along the second dimension correspond to raw unnormalized scores for each class. The CrossEntropyLoss module does the softmax calculation for us, so we do not need to apply our own softmax to the output of our neural network.\n",
    "- output: The second parameter to CrossEntropyLoss is the true label. It expects an *integer valued* tensor of dimension $(N)$. The integer at each element corresponds to the correct class. In our case, the \"correct\" class labels are class 0, class 1, and class 1.\n",
    "\n",
    "Try out the loss function on three toy predictions. The true class labels are $y=[1,1,0]$. The first two examples correspond to predictions that are \"correct\" in that they have higher raw scores for the correct class. The second example is \"more confident\" in the prediction, leading to a smaller loss. The last two examples are incorrect predictions with lower and higher confidence respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate schedulers\n",
    "\n",
    "Often we do not want to use a fixed learning rate throughout all training. PyTorch offers learning rate schedulers to change the learning rate over time. Common strategies include multiplying the lr by a constant every epoch (e.g. 0.9) and halving the learning rate when the training loss flattens out.\n",
    "\n",
    "See the [learning rate scheduler docs](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for usage and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "When working with images, we often want to use convolutions to extract features using convolutions. PyTorch implments this for us in the `torch.nn.Conv2d` module. It expects the input to have a specific dimension $(N, C_{in}, H_{in}, W_{in})$ where $N$ is batch size, $C_{in}$ is the number of channels the image has, and $H_{in}, W_{in}$ are the image height and width respectively.\n",
    "\n",
    "We can modify the convolution to have different properties with the parameters:\n",
    "- kernel_size\n",
    "- stride\n",
    "- padding\n",
    "\n",
    "They can change the output dimension so be careful.\n",
    "\n",
    "See the [`torch.nn.Conv2d` docs](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links:\n",
    "- [60 minute PyTorch Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [PyTorch Docs](https://pytorch.org/docs/stable/index.html)\n",
    "- [Lecture notes on Auto-Diff](https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Custom Datasets, DataLoaders\n",
    "===================================================\n",
    "This is modified from pytorch official tutorial.\n",
    "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
    "\n",
    "A lot of effort in solving any machine learning problem goes in to\n",
    "preparing the data. PyTorch provides many tools to make data loading\n",
    "easy and hopefully, to make your code more readable. In this tutorial,\n",
    "we will see how to load and preprocess/augment data from a non trivial\n",
    "dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class\n",
    "-------------\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset.\n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get $i$\\ th sample\n",
    "\n",
    "Let's create a dataset class for our face landmarks dataset. We will\n",
    "read the csv in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``. This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict\n",
    "``{'image': image, 'landmarks': landmarks}``. Our dataset will take an\n",
    "optional argument ``transform`` so that any required processing can be\n",
    "applied on the sample. We will see the usefulness of ``transform`` in the\n",
    "next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed Presision Training\n",
    "===================================================\n",
    "**Author**: `Chi-Liang Liu <https://liangtaiwan.github.io>`\n",
    "**Ref**: https://github.com/NVIDIA/apex\n",
    "Using mixed precision to train your networks can be:\n",
    "- 2-4x faster\n",
    "- memory-efficient\n",
    "in only 3 lines of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apex \n",
    "\n",
    "NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch. Some of the code here will be included in upstream Pytorch eventually. The intention of Apex is to make up-to-date utilities available to users as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apex.amp\n",
    "\n",
    "Amp allows users to easily experiment with different pure and mixed precision modes.\n",
    "Commonly-used default modes are chosen by\n",
    "selecting an \"optimization level\" or ``opt_level``; each ``opt_level`` establishes a set of\n",
    "properties that govern Amp's implementation of pure or mixed precision training.\n",
    "Finer-grained control of how a given ``opt_level`` behaves can be achieved by passing values for\n",
    "particular properties directly to ``amp.initialize``.  These manually specified values\n",
    "override the defaults established by the ``opt_level``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f5e8b68ab591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Declare model and optimizer as usual, with default (FP32) precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apex'"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "\n",
    "# Declare model and optimizer as usual, with default (FP32) precision\n",
    "model = torch.nn.Linear(10, 100).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Allow Amp to perform casts as required by the opt_level\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "...\n",
    "# loss.backward() becomes:\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-courses_ML19",
   "language": "python",
   "name": "python3-courses_ml19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
