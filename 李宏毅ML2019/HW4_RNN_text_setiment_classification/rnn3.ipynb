{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83eb53b7",
   "metadata": {},
   "source": [
    "改进3：\n",
    "\n",
    "1. 数据集去除标点符号\n",
    "\n",
    "利用re库去除`.,?!`等标点和数字0-9\n",
    "\n",
    "2. 模型改为BiLSTM+Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb57acb",
   "metadata": {},
   "source": [
    "# util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5483b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练集方法\n",
    "def load_training_data(path=\"training_label.txt\"):\n",
    "    \"\"\"\n",
    "    输出每句话的分词（按英文空格分）列表 和 标签列表\n",
    "    \"\"\"\n",
    "    # 带label的训练集\n",
    "    if \"training_label\" in path:\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines= [line.strip('\\n')for line in lines]\n",
    "        x = [line[10:] for line in lines]\n",
    "        x = [re.sub(r\"([.,?!'])\", r\"\", line) for line in lines]\n",
    "        x = [' '.join(s.split()) for s in x]\n",
    "        x = [s.split() for s in x]\n",
    "        y = [line[0] for line in lines]\n",
    "        return x, y\n",
    "    # 不带label的训练集  \n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n') for line in lines]\n",
    "        x = [re.sub(r\"([.,?!'\\d])\", r\"\", line) for line in lines]\n",
    "        x = [' '.join(s.split()) for s in x]\n",
    "        x = [s.split() for s in x]\n",
    "        return x\n",
    "    \n",
    "# 加载测试集方法\n",
    "def load_testing_data(path=\"testing_data.txt\"):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "#         X = [','.join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]]\n",
    "        X = [line.strip('\\n') for line in lines]\n",
    "    X = [re.sub(r\"([,.?!'])\", r\"\", s) for s in lines]\n",
    "    X = [\" \".join(s.split()) for s in X]\n",
    "    X = [s.split() for s in X]\n",
    "    return X\n",
    "\n",
    "# 评估方法\n",
    "def evaluation(outputs, labels):\n",
    "    \"\"\"\n",
    "    outputs 和 labels是tensor类型。\n",
    "    sigmoid激活后输出，所以outputs的范围是(0,1)。\n",
    "    \"\"\"\n",
    "    outputs[outputs>=0.5] = 1\n",
    "    outputs[outputs<0.5] = 0\n",
    "    correct = torch.sum(torch.eq(outputs,labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08a8c5",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e80dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading traning data...\n",
      "loading training nolabel data...\n",
      "loading testing data...\n",
      "saving model...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import re\n",
    "def train_word2vec(x):\n",
    "    model = word2vec.Word2Vec(x, vector_size=250, min_count=5, window=5, workers=12, epochs=10, sg=1)  # sg=1 为Skip-Gram, sg=0(默认) 为CBOW。\n",
    "    return model\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading traning data...\")\n",
    "    x_train, y_train = load_training_data(\"training_label.txt\")\n",
    "    print(\"loading training nolabel data...\")\n",
    "    x_train_no_label = load_training_data(\"training_nolabel.txt\")\n",
    "    print(\"loading testing data...\")\n",
    "    x_test = load_testing_data(\"testing_data.txt\")\n",
    "    wv_model = train_word2vec(x_train + x_train_no_label + x_test)\n",
    "    print(\"saving model...\")\n",
    "    wv_model.save(\"w2v_all.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fa25f",
   "metadata": {},
   "source": [
    "\n",
    "# data prepropress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e78ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "class Prepropress:\n",
    "    def __init__(self, sen_len, w2v_path=\"w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "#         self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    \n",
    "    def get_w2v_model(self):\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)  # 加载词向量预训练模型\n",
    "        self.embedding_size = self.embedding.vector_size\n",
    "        \n",
    "    def add_embedding(self, word):\n",
    "        # 将<PAD> 和 <UNK> 添加到embedding_matrix, 为这两个word赋予一个随机的向量\n",
    "        vector = torch.empty(1, self.embedding_size)\n",
    "        nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "#         self.embedding_matrix.append(vector)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], dim=0)\n",
    "        \n",
    "    def make_embedding(self, load=True):\n",
    "        if load:\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "#         for i, word in enumerate(self.embedding.wv.key_to_index.key()):\n",
    "#             self.word2idx[word] = len(self.word2idx)\n",
    "#             self.idx2word.append(word)\n",
    "#             self.embedding_matrix.append(self.embedding[word])\n",
    "        self.word2idx = self.embedding.wv.key_to_index\n",
    "        self.idx2word = self.embedding.wv.index_to_key\n",
    "        self.embedding_matrix = self.embedding.wv.vectors\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 添加<pad>和<unk>\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) < self.sen_len:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "#             print(f'pad_len:{pad_len}')\n",
    "            sentence += [self.word2idx['<PAD>']] * pad_len\n",
    "#             print(f'sentence: {sentence}')\n",
    "        else:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "#         print(f'len(sentence):{len(sentence)}')\n",
    "#         print(f'self.sen_len: {self.sen_len}')\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    def sentence_word2idx(self, sentences):\n",
    "        # 把句子中的单词转换为index\n",
    "        sentence_list = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence_i = []\n",
    "            for word in sentence:\n",
    "#                 if word in self.word2idx:\n",
    "#                     word_vector = self.embedding_matrix(self.word2idx[word])\n",
    "#                 else:\n",
    "#                     word_vector = self.embedding_matrix(self.word2idx['<UNK>'])\n",
    "                if word in self.word2idx:\n",
    "                    sentence_i.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_i.append(self.word2idx['<UNK>'])\n",
    "#                 sentence_i.append(word_vector)\n",
    "            sentence_i = self.pad_sentence(sentence_i)\n",
    "            sentence_list.append(sentence_i)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    \n",
    "    def labels_to_tensor(self, y):\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5eac0",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d4139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class MyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    将数据包装为Dataset类，然后传入DataLoader，使用DataLoader这个类对数据操作\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.data[index], self.label[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0cf2a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ce4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Atten_BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, n_layer, dropout, fix_embedding):\n",
    "        super(Atten_BiLSTM, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = nn.Parameter(embedding)  # 将一个变量转换为可训练的参数，并将这个参数绑定到module的net.parameter()中\n",
    "        self.embedding.weight.reqiure_grad = False if fix_embedding else True  # 是否将embedding fix住，如果fix_embedding为False，那么在训练过程中embedding也会跟着训练\n",
    "        # 疑问：embeddinxxg.size(1)和embedding_dim不一样吗？\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layer, batch_first=True,\n",
    "                           bidirectional=True)\n",
    "        self.classifer = nn.Sequential(nn.Dropout(dropout),\n",
    "                                       \n",
    "                                      nn.Linear(hidden_dim, 64),\n",
    "                                      nn.Dropout(dropout),\n",
    "                                     \n",
    "                                      nn.Linear(64, 32),\n",
    "                                      nn.Dropout(dropout),\n",
    "                                       \n",
    "                                       nn.Linear(32, 16),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       \n",
    "                                       nn.Linear(16, 1),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       \n",
    "                                       nn.Sigmoid())\n",
    "        self.attention_layer = nn.Sequential(nn.Lienar(hhidden_dim, hidden_dim),\n",
    "                                            nn.ReLU())\n",
    "    \n",
    "    def attention(self,  output, hidden):\n",
    "        \"\"\"\n",
    "        output (batch_size, seq_len, hidden_size * num_direction), rnn最后一层的输出。\n",
    "        hidden (batch_size, num_layers * num_directions, hidden_size)，rnn最后一个时间步的隐状态\n",
    "        \"\"\"\n",
    "        # batch_size, seq_len, hidden_size , 不明白这里为什么相加？相加的结果是什么？\\\n",
    "        print(f'output: {output}')\n",
    "        output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "        print(f'output: {output}')\n",
    "        \n",
    "        print(f'hidden: {hidden}')\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        print(f'hidden: {hidden}')\n",
    "        hidden = hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        print(f'hidden: {hidden}')\n",
    "        \n",
    "        atten_w = self.attention_layer(hidden)   # (batch_size, 1, hidden_size)\n",
    "        m = nn.Tanh()(output)  # (batch_size, seq_len, hidden_size)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        \n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, output)\n",
    "        \n",
    "        return context.squeeze(1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        \n",
    "        # x (batch, seq_len, hidden_size)\n",
    "        # hidden (num_layers * num_direction, batch_size, hidden_size)\n",
    "        x, (hidden, _) = self.lstm(inputs, None)\n",
    "        hidden = hidden.permute(1, 0, 2)  # (batch_size, num_layers * num_direction, hidden_size) \n",
    "        \n",
    "        #atten_out [batch_size, 1, hidden_dim]\n",
    "        atten_out = self.attention(x, hidden)\n",
    "        x = self.classifer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca557c6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23bf6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import BCELoss\n",
    "# from torch.nn.function import binary_cross_entropy\n",
    "import torch.optim as optim \n",
    "\n",
    "def add_label(outputs, threshold=0.9):\n",
    "    id = (outputs>=threshold) | (outputs<1-threshold)\n",
    "    outputs[outputs>=threshold] = 1 # 大于等于 threshold 为正面\n",
    "    outputs[outputs<1-threshold] = 0 # 小于 threshold 为负面\n",
    "    return outputs.long(), id\n",
    "\n",
    "def train(X_train, y_train, val, train_no_label, epoches, model, lr, batch_size, device):\n",
    "    \"\"\"\n",
    "    训练集train, 验证集val，轮数epoches，模型model，学习率lr, 批大小batch_size\n",
    "    \"\"\"\n",
    "    # 损失函数\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"total parameters: {} ; trainable parameters: {}\".format(total, trainable))\n",
    "    criterion = BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     t_batches = len(train)\n",
    "#     v_batches = len(val)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epoches):\n",
    "        train_dataset = MyDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=0)\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # 去掉张量内元素的外层的所有中括号\n",
    "            loss = criterion(outputs, labels)  # 计算损失\n",
    "            loss.backward()  # 反向传播梯度\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = evaluation(outputs, labels)\n",
    "#             # end = '\\r' 滚动输出到屏幕\n",
    "#             print(\"[Epoch_{} {}/{}], loss: {:.3f}, acc: {:.3f}\".format(\n",
    "#                 epoch, i, t_batches, loss.item(), acc/batch_size*100), end=\"\\r\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc / batch_size\n",
    "        t_batches = len(train_loader)\n",
    "        print(\"[Epoch_{}], total_loss: {:.3f}, total_acc: {:.3f}\".format(\n",
    "            epoch, total_loss / t_batches, total_acc / t_batches * 100))\n",
    "\n",
    "        model.eval()\n",
    "        # self-training\n",
    "        if epoch >= 4:\n",
    "            train_no_label_dataset = MyDataset(train_no_label, None)\n",
    "            train_no_label_loader = DataLoader(train_no_label_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0)\n",
    "            with torch.no_grad():\n",
    "                for i, (inputs) in enumerate(train_no_label_loader):\n",
    "                    inputs = inputs.to(device, dtype=torch.long)\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.squeeze()  # 去掉最外面的dimension, 方便让outputs丢进loss()\n",
    "                    labels, id = add_label(outputs)\n",
    "                    # 加入新标注的数据\n",
    "                    X_train = torch.cat((X_train.to(device), inputs[id]), dim=0)\n",
    "                    y_train = torch.cat((y_train.to(device), labels[id]), dim=0)\n",
    "                    if i == 0:\n",
    "                        train_no_label = inputs[~id]\n",
    "                    else:\n",
    "                        train_no_label = torch.cat((train_no_label.to(device), \n",
    "                                                    inputs[~id]), dim=0)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(val):\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                labels  = labels.to(device, dtype=torch.float)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                acc = evaluation(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc / batch_size\n",
    "            v_batches = len(val)\n",
    "            print(\"Valid:[Epoch_{}], total_loss: {:.3f}, total_acc: {:.3f}\".format(\n",
    "                epoch, total_loss / v_batches, total_acc / v_batches * 100))\n",
    "            if total_acc > best_acc:\n",
    "#                 print(\"best_acc: {:.3f}\".format(best_acc))\n",
    "                best_acc = total_acc\n",
    "                torch.save(model, 'ckpt_rnn1.model')\n",
    "                print(f'saving model with acc {total_acc/v_batches*100 :.3f}')\n",
    "        print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e6eaf",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dedb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def test(test_x, model, batch_size, device):\n",
    "    model.eval()\n",
    "    res_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_x):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            res_list += outputs.int().tolist()\n",
    "    return res_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e958708",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess data ... ...\n",
      "dataset ... ...\n",
      "dataloader ... ...\n",
      "traning ... ...\n",
      "total parameters: 14195679 ; trainable parameters: 14195679\n",
      "[Epoch_0], total_loss: 0.574, total_acc: 67.107\n",
      "Valid:[Epoch_0], total_loss: 0.464, total_acc: 77.543\n",
      "saving model with acc 77.543\n",
      "---------------------------------------------\n",
      "[Epoch_1], total_loss: 0.457, total_acc: 78.618\n",
      "Valid:[Epoch_1], total_loss: 0.449, total_acc: 78.374\n",
      "saving model with acc 78.374\n",
      "---------------------------------------------\n",
      "[Epoch_2], total_loss: 0.434, total_acc: 79.989\n",
      "Valid:[Epoch_2], total_loss: 0.443, total_acc: 78.717\n",
      "saving model with acc 78.717\n",
      "---------------------------------------------\n",
      "[Epoch_3], total_loss: 0.418, total_acc: 80.859\n",
      "Valid:[Epoch_3], total_loss: 0.441, total_acc: 78.697\n",
      "---------------------------------------------\n",
      "[Epoch_4], total_loss: 0.403, total_acc: 81.737\n",
      "Valid:[Epoch_4], total_loss: 0.448, total_acc: 78.837\n",
      "saving model with acc 78.837\n",
      "---------------------------------------------\n",
      "[Epoch_5], total_loss: 0.119, total_acc: 95.441\n",
      "Valid:[Epoch_5], total_loss: 0.511, total_acc: 79.125\n",
      "saving model with acc 79.125\n",
      "---------------------------------------------\n",
      "[Epoch_6], total_loss: 0.102, total_acc: 96.459\n",
      "Valid:[Epoch_6], total_loss: 0.547, total_acc: 79.160\n",
      "saving model with acc 79.160\n",
      "---------------------------------------------\n",
      "[Epoch_7], total_loss: 0.095, total_acc: 96.911\n",
      "Valid:[Epoch_7], total_loss: 0.618, total_acc: 79.051\n",
      "---------------------------------------------\n",
      "[Epoch_8], total_loss: 0.091, total_acc: 97.247\n",
      "Valid:[Epoch_8], total_loss: 0.683, total_acc: 79.140\n",
      "---------------------------------------------\n",
      "[Epoch_9], total_loss: 0.089, total_acc: 97.450\n",
      "Valid:[Epoch_9], total_loss: 0.685, total_acc: 79.021\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 设置运算的设备\n",
    "\n",
    "# - 运行word2vec模块，由训练集和测试集训练生成w2v.model预训练模型。\n",
    "# - 处理预训练的词向量，生成word2idx, idx2word, embedding_matrix。处理数据集，将句子处理成word2idx的格式，每个句子一个列表，并转为张量；将label处理成列表并转为张量。\n",
    "train_x, train_y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "sen_len = 20\n",
    "batch_size = 128\n",
    "epoches=15\n",
    "lr = 0.0001\n",
    "w2v_path = \"w2v_all.model\"\n",
    "\n",
    "print(f'preprocess data ... ...')\n",
    "preprocess = Prepropress(sen_len=sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "\n",
    "train_x = preprocess.sentence_word2idx(train_x)\n",
    "train_y = preprocess.labels_to_tensor(train_y)\n",
    "\n",
    "train_x_no_label = preprocess.sentence_word2idx(train_x_no_label)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.1, random_state=1, stratify=train_y)\n",
    "\n",
    "# 将数据集包装成dataset类\n",
    "print(f'dataset ... ...')\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "# 将数据集分批成dataloader类\n",
    "print(f'dataloader ... ...')\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "model = LSTM_net(embedding=embedding, embedding_dim=250, hidden_dim=150, n_layer=1, \n",
    "                 dropout=0.5, fix_embedding=True)\n",
    "model = model.to(device)\n",
    "print(f'traning ... ...')\n",
    "train(X_train=X_train, y_train=y_train, val=val_loader, train_no_label=train_x_no_label, \n",
    "      epoches=epoches, model=model, lr=lr, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c65ae",
   "metadata": {},
   "source": [
    "# predict and write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b38da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ... ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Prepropress' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-044884f30627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loading testing data ... ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_testing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing_data.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrepropress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_word2idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Prepropress' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "print(f'loading testing data ... ...')\n",
    "test_x = load_testing_data('testing_data.txt')\n",
    "preprocess = Prepropress(sen_len=50)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx(test_x)\n",
    "test_dataset = MyDataset(test_x, None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, \n",
    "                                          shuffle=False, num_workers=8)\n",
    "\n",
    "print(f'loading model ... ...')\n",
    "model = torch.load('ckpt_rnn1.model')\n",
    "outputs = test(test_loader, model, batch_size=batch_size, device=device)\n",
    "\n",
    "# 保存为csv\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))], \"label\": outputs})\n",
    "print(f'saving csv ... ...')\n",
    "tmp.to_csv('predict_rnn1.csv', index=False)\n",
    "print(f'finish predicting!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd177d",
   "metadata": {},
   "source": [
    "# 结果\n",
    "\n",
    "[Epoch_6], total_loss: 0.102, total_acc: 96.459\n",
    "\n",
    "Valid:[Epoch_6], total_loss: 0.547, total_acc: 79.160\n",
    "\n",
    "saving model with acc 79.160\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7554dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85137043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf4da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c076a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_main",
   "language": "python",
   "name": "venv_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
